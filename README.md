# AI.zip üóúÔ∏è

## Overview üîé

Some notes on AI as compression.

## References üîé

### Articles üìë

* (Bennet *et al.*, 1998) [Information Distance](https://cs.uwaterloo.ca/~mli/informationdistance.pdf)
* (Buttrick, 2024) [Studying large language models as compression algorithms for human culture](https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00001-9)
* (Chen *et al.*, 2024) [Information Compression in the AI Era: Recent Advances and Future Challenges](https://arxiv.org/abs/2406.10036)
* (Del√©tang *et al.*, 2023) [Language Modeling is Compression](https://arxiv.org/pdf/2309.10668)
* (Frank, Chui & Witten, 2000) [Text categorization using compression models](https://ieeexplore.ieee.org/document/838202/)
* (Goldblum *et al.*, 2023) [The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning](https://arxiv.org/abs/2304.05366)
* (Hinton & Camp, 1993) [Keeping the neural networks simple by minimizing the description length of the weights](https://dl.acm.org/doi/10.1145/168304.168306)
* (Jiang *et al.*, 2023) ["Low-Resource" Text Classification: A Parameter-Free Classification Method with Compressors](https://aclanthology.org/2023.findings-acl.426)
  - Preprint // [Less is More: Parameter-Free Text Classification with Gzip](https://arxiv.org/abs/2212.09410)
* (Lan *et al.*, 2022) [Minimum Description Length Recurrent Neural Networks](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00489/112499/Minimum-Description-Length-Recurrent-Neural)
* (Li *et al.*, 2025) [Lossless data compression by large models](https://www.nature.com/articles/s42256-025-01033-7)
* (Maguire *et al.*, 2015) [Compressionism: A Theory of Mind Based on Data Compression](https://norma.ncirl.ie/2114/)
* (Mittu *et al.*, 2024) [FineZip: Pushing the Limits of Large Language Models for Practical Lossless Text Compression](https://arxiv.org/abs/2409.17141v1)
* (Pan *et al.*, 2025) [Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws](https://arxiv.org/abs/2504.09597)
* (Rao, 2025) [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
* (Schmidhuber, 1997) [Discovering Neural Nets with Low Kolmogorov Complexity and High Generalization Capability](https://pubmed.ncbi.nlm.nih.gov/12662875/)
* (Sculley & Brodley, 2006) [Compression and machine learning: a new perspective on feature space vectors](https://www.semanticscholar.org/paper/Compression-and-machine-learning%3A-a-new-perspective-Sculley-Brodley/70e8e1457aadbee439d47a2fe071007b1cf1dece)
* (Teahan & Harper, 2003) [Using compression-based language models for text categorization](https://boston.lti.cs.cmu.edu/callan/Workshops/lmir01/WorkshopProcs/Papers/teahan.pdf)
* (Valmeekam *et al.*, 2023) [LLMZip: Lossless Text Compression using Large Language Models](https://arxiv.org/abs/2306.04050)
* (Yang, Mandt & Theis, 2022) [An Introduction to Neural Data Compression](https://arxiv.org/abs/2202.06533)
* (Yu *et al.*, 2023) [White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?](https://arxiv.org/abs/2311.13110)
* (Ziv & LeCun, 2024) [To Compress or Not to Compress‚ÄîSelf-Supervised Learning and Information Theory: A Review](https://www.mdpi.com/1099-4300/26/3/252)

### Books üìö

* (MacKay, 2003) [Information Theory, Inference and Learning Algorithms](https://www.cambridge.org/gb/universitypress/subjects/computer-science/pattern-recognition-and-machine-learning/information-theory-inference-and-learning-algorithms)
* (Mohri, Rostamizadeh & Talwalkar, 2018) [Foundations of Machine Learning](https://mitpress.mit.edu/9780262039406/foundations-of-machine-learning/)
* (Shannon & Weaver, 1949) [Mathematical Theory of Communication](https://web.archive.org/web/20000823215030/http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf)

### Blogs/News ‚úçÔ∏è

* (Andrew's Blog, 2024) [Using an LLM for text compression](https://blog.cleverdomain.org/using-an-llm-for-text-compression)
* (ArsTechnica, 2023) [AI language models can exceed PNG and FLAC in lossless compression, says study](https://arstechnica.com/information-technology/2023/09/ai-language-models-can-exceed-png-and-flac-in-lossless-compression-says-study/)
* (Bactra, 2023) ["Attention", "Transformers", in Neural Network "Large Language Models"](http://bactra.org/notebooks/nn-attention-and-transformers.html)
* (Confessions of a Code Addict, 2023) [How Language Models Beat PNG and FLAC Compression & What It Means](https://blog.codingconfessions.com/p/language-modeling-is-compression)
* (Hackaday, 2023) [Text compression gets weirdly efficient with LLMs](https://hackaday.com/2023/08/27/text-compression-gets-weirdly-efficient-with-llms/)
* (Hendrick Erz, 2023) [Why gzip just beat a Large Language Model](https://www.hendrik-erz.de/post/why-gzip-just-beat-a-large-language-model)
* (IEEE, 2023) [Intelligence via Compression of Information](https://www.computer.org/publications/tech-news/community-voices/intelligence-via-compression-of-information)
* (LSE, 2023) [Compression and complexity: Making sense of Artificial Intelligence](https://blogs.lse.ac.uk/europpblog/2023/06/30/compression-and-complexity-making-sense-of-artificial-intelligence/)
* (MaximumCompression, 2025) [AI and File Compression: How Artificial Intelligence Is Shaping the Future of Data Reduction](https://www.maximumcompression.com/ai-and-file-compression-how-artificial-intelligence-is-shaping-the-future-of-data-reduction/)
* (TechXplore, 2025) [Algorithm based on LLMs doubles lossless data compression rates](https://techxplore.com/news/2025-05-algorithm-based-llms-lossless-compression.html)
* (The New Yorker, 2023) [ChatGPT is a blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web)

### Code üíª

* [`simple-KNN-gzip`](https://github.com/Sentdex/Simple-kNN-Gzip) - 
* [`npc_gzip`](https://github.com/bazingagin/npc_gzip) - code for Jiang *et al.* (2023)
* Ballard's stuff
  - [`nccp`](https://bellard.org/nncp/) - an experiment to build a practical lossless data compressor with neural networks
  - [`ts_zip`](https://bellard.org/ts_zip/) - utility that can compress and hopefully decompress text files using an LLM (current version uses [RWKV](https://www.rwkv.com/))
  - [`ts_sms`](https://bellard.org/ts_sms/) - short message compression using an LLM

### Videos/Podcasts üéûÔ∏è

* (Andrej Karpathy, 2023) [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
* (Lex Fridman, 2021) [Hutter Prize: Intelligence as Compression](https://www.youtube.com/watch?v=boiW5qhrGH4) with George Hotz
* (sentdex, 2023) [Gzip is all You Need!](https://www.youtube.com/watch?v=jkdWzvMOPuo)
* (StackOverflow, 2024) [Compression is understanding](https://stackoverflow.blog/2024/01/26/compression-is-understanding/)
* (Stanford MLSys, 2023) [Compression for AGI](https://www.youtube.com/watch?v=dO4TPJkeaaU) by Jack Rae

### Miscellaneous üëæ

* [Data Compression Explained](https://mattmahoney.net/dc/dce.html) by Matt Mahoney
* [Prize for Compressing Human Knowledge](http://prize.hutter1.net/index.htm) AKA Hutter Prize
  - Task // Compress the `1GB` file `enwik9` to less than the current record of about `110MB`
  - FAQ // [What is (artificial) intelligence?](http://prize.hutter1.net/hfaq.htm#ai), [What does compression have to do with (artificial) intelligence?](http://prize.hutter1.net/hfaq.htm#compai), and [Why is "understanding" of the text or "intelligence" needed to achieve maximal compression?](http://prize.hutter1.net/hfaq.htm#understand)
* [Arithmetic Coding](https://en.wikipedia.org/wiki/Arithmetic_coding) on Wikipedia
* [Data Compression](https://serp.ai/posts/data-compression/) by Serp.ai
* Live from Redditverse
   - [AI is just compression?](https://www.reddit.com/r/AskComputerScience/comments/um18by/ai_is_just_compression/) at r/AskComputerScience
* Random fun stuff
    - [Data Quality](https://www.explainxkcd.com/wiki/index.php/2739:_Data_Quality) by xkcd
    - [Xerox scanners/photocopiers randomly alter numbers in scanned documents](https://www.dkriesel.com/en/blog/2013/0802_xerox-workcentres_are_switching_written_numbers_when_scanning) by David Kriesel